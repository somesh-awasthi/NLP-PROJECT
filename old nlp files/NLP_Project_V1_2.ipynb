{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZhGNMbKzBAzKbBA6wLcQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somesh-awasthi/NLP-PROJECT/blob/main/NLP_Project_V1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5z54E_WzlbFU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain-openai\n",
        "!pip install ctransformers sentence-transformers langchain-chroma\n",
        "!pip install pandas nltk spacy PyPDF\n",
        "%pip install --upgrade --quiet  sentence-transformers langchain-chroma langchain langchain-openai > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to database\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "path=\"/content/drive/MyDrive/Colab Notebooks/data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaE3VcfomVeZ",
        "outputId": "7eddf698-c949-4db1-fef2-206005c12b67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "# Load documents from PDF\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "u3BAHZzz0m2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d14e92-204a-48a6-8a1b-9298d0301429"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 124 0 (offset 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization and POS tagging using SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Filtering out tokens based on POS tags and dependency parsing\n",
        "    filtered_tokens = [token.text.lower() for token in doc if token.pos_ not in [\"SPACE\", \"X\"] and token.dep_ not in [\"det\", \"punct\"]]\n",
        "\n",
        "    # Stopword removal\n",
        "    filtered_tokens = [token for token in filtered_tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens))]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxMvgdYRmqYf",
        "outputId": "c95a122e-874d-4bc4-becc-f9fa50046fae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess each document\n",
        "for doc in documents:\n",
        "    doc.page_content = preprocess_text(doc.page_content)"
      ],
      "metadata": {
        "id": "QoMCXfI_1Ret"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Split the preprocessed documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    add_start_index=True,\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhv2UMbK214-",
        "outputId": "d0f0a9c2-ed71-4faa-c748-7f798c734e1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 3263 documents into 13402 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessed chunks to Chroma\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
        "db = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=\"./drive/MyDrive/Colab Notebooks/chroma_db\")"
      ],
      "metadata": {
        "id": "T5boCGcP3HyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb671d35-67dd-4197-a92f-81ebc7f3637c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YghKekBDFtrT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain, StuffDocumentsChain\n",
        "from langchain_community.document_transformers import (\n",
        "    LongContextReorder,\n",
        ")\n",
        "# Start conversation loop\n",
        "context_text = \"\"\n",
        "while True:\n",
        "    query_text = input(\"Enter your query (type 'quit' to exit): \")\n",
        "\n",
        "    if query_text.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Search the DB.\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=7)\n",
        "\n",
        "    # Reorder documents\n",
        "    reordering = LongContextReorder()\n",
        "    reorder_docs = reordering.transform_documents(results)\n",
        "\n",
        "    if len(reorder_docs) == 0 or reorder_docs[0][1] < 0.7:\n",
        "        print(f\"Unable to find matching results.\")\n",
        "        continue\n",
        "\n",
        "    new_context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in reorder_docs])\n",
        "    context_text += \"\\n\\n---\\n\\n\" + new_context_text\n",
        "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "    # print(prompt)\n",
        "\n",
        "    model = ChatOpenAI()\n",
        "    response_text = model.invoke(prompt)\n",
        "\n",
        "    # Load the model\n",
        "    sources = [doc.metadata.get(\"source\", None) for doc, _score in reorder_docs]\n",
        "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "    print(formatted_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwCRUBnGZC4r",
        "outputId": "1080e20c-8339-46ff-975f-0280fb2af6f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query (type 'quit' to exit): I'm having fever since two weeks, also sometimes vomiting with watery eyes, what could be the disease?\n",
            "Response: content='Based on the symptoms described in the context, the possible disease could be viral conjunctivitis caused by adenovirus.' response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 1485, 'total_tokens': 1510}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None} id='run-dd7f7baf-6c87-4ff6-b4e1-3ace22710114-0'\n",
            "Sources: ['/content/drive/MyDrive/Colab Notebooks/data/medical-diagnosis.pdf', '/content/drive/MyDrive/Colab Notebooks/data/medical-diagnosis.pdf', '/content/drive/MyDrive/Colab Notebooks/data/Medical_book.pdf', '/content/drive/MyDrive/Colab Notebooks/data/Medical_book.pdf', '/content/drive/MyDrive/Colab Notebooks/data/medical-diagnosis.pdf', '/content/drive/MyDrive/Colab Notebooks/data/medical-diagnosis.pdf', '/content/drive/MyDrive/Colab Notebooks/data/medical-diagnosis.pdf']\n",
            "Enter your query (type 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}